# Optimized Docker Compose for Production (Hetzner CPX31: 4 CPU, 8GB RAM)
version: "3.9"

services:
  # Main Bot
  bot:
    build:
      context: .
      dockerfile: Dockerfile.bot
    container_name: chopsticks-bot
    restart: unless-stopped
    env_file: .env
    environment:
      - STORAGE_DRIVER=postgres
      - NODE_ENV=production
    volumes:
      - ./data:/app/data
    networks:
      - chopsticks
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      lavalink:
        condition: service_healthy
    mem_limit: 512m
    mem_reservation: 256m
    cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Agent Runner (5-10 agents)
  agents:
    build:
      context: .
      dockerfile: Dockerfile.agentrunner
    container_name: chopsticks-agents
    restart: unless-stopped
    env_file: .env
    environment:
      - STORAGE_DRIVER=postgres
      - NODE_ENV=production
      - AGENT_RUNNER_POLL_INTERVAL_MS=10000
      - MAX_AGENTS_PER_RUNNER=10
    networks:
      - chopsticks
    depends_on:
      postgres:
        condition: service_healthy
      lavalink:
        condition: service_healthy
      bot:
        condition: service_started
    mem_limit: 1g
    mem_reservation: 512m
    cpus: '1.5'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: chopsticks-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-chopsticks}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_DB: ${POSTGRES_DB:-chopsticks}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - chopsticks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: '0.5'
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=2MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_connections=100
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chopsticks"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: chopsticks-redis
    restart: unless-stopped
    command: >
      redis-server
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - chopsticks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: '0.25'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  # Lavalink Music Server
  lavalink:
    image: ghcr.io/lavalink-devs/lavalink:4
    container_name: chopsticks-lavalink
    restart: unless-stopped
    environment:
      - _JAVA_OPTIONS=-Xmx1G -Xms512M
      - SERVER_PORT=2333
      - LAVALINK_SERVER_PASSWORD=${LAVALINK_PASSWORD:-youshallnotpass}
    volumes:
      - ./lavalink/application.yml:/opt/Lavalink/application.yml:ro
      - ./lavalink/plugins:/opt/Lavalink/plugins:ro
    networks:
      - chopsticks
    mem_limit: 1536m
    mem_reservation: 768m
    cpus: '1.5'
    healthcheck:
      test: ["CMD-SHELL", "curl -f -H 'Authorization: youshallnotpass' http://localhost:2333/version || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Dashboard (Optional - lightweight)
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.bot
      args:
        - BUILD_TARGET=dashboard
    container_name: chopsticks-dashboard
    restart: unless-stopped
    env_file: .env
    environment:
      - NODE_ENV=production
      - STORAGE_DRIVER=postgres
    command: ["npm", "run", "dashboard"]
    # Ports exposed via Caddy, but kept for direct access if needed
    ports:
      - "8788:8788"
    networks:
      - chopsticks
    depends_on:
      - postgres
      - redis
    mem_limit: 256m
    mem_reservation: 128m
    cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    profiles: ["dashboard"]  # Only start if explicitly requested

  # Caddy Reverse Proxy
  caddy:
    image: caddy:2-alpine
    container_name: chopsticks-caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - chopsticks
    depends_on:
      - dashboard
    mem_limit: 128m
    cpus: '0.25'
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    profiles: ["dashboard"]

  # Prometheus Metrics (Optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: chopsticks-prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - chopsticks
    ports:
      - "9090:9090"
    mem_limit: 256m
    cpus: '0.25'
    profiles: ["monitoring"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  caddy_data:
    driver: local
  caddy_config:
    driver: local
  prometheus_data:
    driver: local

networks:
  chopsticks:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
